{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times Now Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def times_now_scraper():\n",
    "    URL=\"https://www.timesnownews.com/\"\n",
    "    r=requests.get(URL)\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')  \n",
    "    heads={}\n",
    "    count=0\n",
    "    for row in soup.findAll('div', attrs = {'class':'component_1'}):\n",
    "        count+=1\n",
    "        head=row.text\n",
    "        words=head.split(\" \")\n",
    "        head=\"\"\n",
    "        for i in words:\n",
    "            if i.isalnum():\n",
    "                head+=i+\" \"\n",
    "            \n",
    "        heads[head]={}\n",
    "        heads[head]['Source']='Times_Now'\n",
    "        heads[head]['link']=URL+row.a['href'][1:]\n",
    "        \n",
    "        if count==15:\n",
    "            break\n",
    "    row=soup.find('div', attrs = {'class':'component_4'})\n",
    "    h=row.find('h2', attrs = {'class':'content'})\n",
    "    #print(h)   \n",
    "    head=h.text\n",
    "    words=head.split(\" \")\n",
    "    head=\"\"\n",
    "    for i in words:\n",
    "            if i.isalnum():\n",
    "                head+=i+\" \"\n",
    "            \n",
    "    heads[head]={}\n",
    "    heads[head]['Source']='Times_Now'\n",
    "    heads[head]['link']=URL+row.a['href'][1:]\n",
    "            \n",
    "    count=0\n",
    "    for row in soup.findAll('div', attrs = {'class':'component_3'}):\n",
    "        count+=1\n",
    "        head=row.text\n",
    "        words=head.split(\" \")\n",
    "        head=\"\"\n",
    "        for i in words:\n",
    "            if i.isalnum():\n",
    "                head+=i+\" \"\n",
    "            \n",
    "        heads[head]={}\n",
    "        heads[head]['Source']='Times_Now'\n",
    "        heads[head]['link']=URL+row.a['href'][1:]\n",
    "        \n",
    "        if count==15:\n",
    "                break\n",
    "    return heads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def extract_news_times(news):\n",
    "    for n in news.keys():\n",
    "        #print(n)\n",
    "        link=news[n]['link']\n",
    "        #print(link)\n",
    "        r=requests.get(link)\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        Briefs=[]\n",
    "        for row in soup.findAll('li', attrs = {'class':'mar-b10'}):\n",
    "            texts=row.text\n",
    "            text_ref=texts[1:len(texts)-1]+'.'\n",
    "            Briefs.append(text_ref)\n",
    "        if Briefs==[]:\n",
    "            Briefs=[soup.find('h1').text]\n",
    "        news[n]['gists']=Briefs\n",
    "        \n",
    "            \n",
    "        row=soup.find('h1')\n",
    "        news[n]['Titles']=[row.text]\n",
    "        date=datetime.today().strftime('%Y-%m-%d')\n",
    "        time=str(datetime.now().time())\n",
    "        \n",
    "        news[n]['Date']=date\n",
    "        news[n]['Time']=time\n",
    "        \n",
    "    return news\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Republic TV scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def republic_tv_scraper():\n",
    "    URL=\"https://www.republicworld.com/\"\n",
    "    r=requests.get(URL)\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')  \n",
    "    heads={}\n",
    "    count=0 \n",
    "    \n",
    "    for row in soup.findAll('div', attrs = {'class':'bdrTop-dddddd masthead-padding hover-effect'}):\n",
    "        flag=0\n",
    "        count+=1\n",
    "        try:\n",
    "            flag=1\n",
    "            h=row.find('h2')\n",
    "            head=h.text\n",
    "            heads[head]={}\n",
    "            heads[head]['Source']='Republic TV'\n",
    "            heads[head]['link']=row.a['href']\n",
    "        except:\n",
    "            i=1\n",
    "            \n",
    "        if flag==0:\n",
    "            try:\n",
    "                h=row.find('h1')\n",
    "                head=h.text\n",
    "                heads[head]={}\n",
    "                heads[head]['Source']='Republic TV'\n",
    "                heads[head]['link']=row.a['href']\n",
    "            except:\n",
    "                i=1\n",
    "        \n",
    "        if count==15:\n",
    "            break\n",
    "    count=0      \n",
    "    for row in soup.findAll('div', attrs = {'class':'bdrTop-dddddd hover-effect pad10 padtop10 padbtm10'}):\n",
    "        count+=1\n",
    "        flag=0\n",
    "        try:\n",
    "            flag=1\n",
    "            h=row.find('h2')\n",
    "            head=h.text\n",
    "            heads[head]={}\n",
    "            heads[head]['Source']='Republic TV'\n",
    "            heads[head]['link']=row.a['href']\n",
    "        except:\n",
    "            i=1\n",
    "        if flag==0:\n",
    "            try:\n",
    "                \n",
    "                h=row.find('h1')\n",
    "                head=h.text\n",
    "                heads[head]={}\n",
    "                heads[head]['Source']='Republic TV'\n",
    "                heads[head]['link']=row.a['href']\n",
    "            except:\n",
    "                i=1\n",
    "        \n",
    "        if count==15:\n",
    "            break\n",
    "    count=0        \n",
    "    for row in soup.findAll('div', attrs = {'class':'hover-effect'}):\n",
    "        count+=1\n",
    "        if count==20:\n",
    "            break\n",
    "        flag=0\n",
    "        \n",
    "        try:\n",
    "            flag=1\n",
    "            h=row.find('h2')\n",
    "            head=h.text\n",
    "            heads[head]={}\n",
    "            heads[head]['Source']='Republic TV'\n",
    "            heads[head]['link']=row.a['href']\n",
    "            continue\n",
    "        except:\n",
    "            i=1\n",
    "        try:\n",
    "            h=row.find('h1')\n",
    "            head=h.text\n",
    "            heads[head]={}\n",
    "            heads[head]['Source']='Republic TV'\n",
    "            heads[head]['link']=row.a['href']\n",
    "        except:\n",
    "            i=1\n",
    "        \n",
    "        \n",
    "            \n",
    "    return heads\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "def get_sentences(article):\n",
    "  extracts=sent_tokenize(article)\n",
    "  sentences=[]\n",
    "  for extract in extracts:\n",
    "    #print(extract)\n",
    "    clean_sentence=extract.replace(\"[^a-zA-Z0-9]\",\" \")   ## Removing special characters\n",
    "    #print(clean_sentence)\n",
    "    obtained=word_tokenize(clean_sentence) \n",
    "    #print(obtained)\n",
    "    sentences.append(obtained)\n",
    "\n",
    "  return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "def get_similarity(sent_1,sent_2,stop_words):\n",
    "  \n",
    "  sent_1=[w.lower() for w in sent_1]\n",
    "  sent_2=[w.lower() for w in sent_2]\n",
    "\n",
    "  total=list(set(sent_1+sent_2)) ## Removing duplicate words in total set\n",
    "\n",
    "  vec_1= [0] * len(total)\n",
    "  vec_2= [0] * len(total)\n",
    "\n",
    "\n",
    "  ## Count Vectorization of two sentences\n",
    "  for w in sent_1:\n",
    "    if w not in stop_words:\n",
    "      vec_1[total.index(w)]+=1\n",
    "\n",
    "  for w in sent_2:\n",
    "    if w not in stop_words:\n",
    "      vec_2[total.index(w)]+=1\n",
    "\n",
    "\n",
    "  return 1-cosine_distance(vec_1,vec_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "def build_matrix(sentences):\n",
    "  stop_words = stopwords.words('english')\n",
    "\n",
    "  sim_matrix=np.zeros((len(sentences),len(sentences)))\n",
    "  ## Adjacency matrix\n",
    "\n",
    "  for id1 in range(len(sentences)):\n",
    "    for id2 in range(len(sentences)):\n",
    "      if id1==id2:  #escaping diagonal elements\n",
    "        continue\n",
    "      else:\n",
    "        sim_matrix[id1][id2]=get_similarity(sentences[id1],sentences[id2],stop_words)\n",
    "\n",
    "  return sim_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(text, eps=0.000001, d=0.85):\n",
    "    score_mat = np.ones(len(text)) / len(text)\n",
    "    delta=1\n",
    "    ### iterative approach\n",
    "    while delta>eps:\n",
    "        score_mat_new = np.ones(len(text)) * (1 - d) / len(text) + d * text.T.dot(score_mat)\n",
    "        delta = abs(score_mat_new - score_mat).sum()\n",
    "        score_mat = score_mat_new\n",
    "\n",
    "    return score_mat_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(article,req=3):\n",
    "  summarized=[]\n",
    "\n",
    "  sentence=get_sentences(article)\n",
    "\n",
    "  sim_matrix=build_matrix(sentence)\n",
    "\n",
    "  score=pagerank(sim_matrix)\n",
    "\n",
    "  ranked_sentence = sorted(((score[i],s) for i,s in enumerate(sentence)), reverse=True)\n",
    "  #print(ranked_sentence[2])\n",
    "  \n",
    "  for i in range(req):\n",
    "      #print(ranked_sentence[i][1])\n",
    "      summarized.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "  return summarized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abhijit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abhijit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Republic text extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def extract_news_rep(news):\n",
    "    for n in news.keys():\n",
    "        #print(n)\n",
    "        link=news[n]['link']\n",
    "        #print(link)\n",
    "        r=requests.get(link)\n",
    "        #print(link)\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        Briefs=[]\n",
    "        sub=soup.find('div',attrs ={'class': 'storypicture'})\n",
    "        \n",
    "        head=soup.find('h1',attrs={'class': 'story-title'})\n",
    "        #h=head.text.replace(',',' ')\n",
    "        news[n]['Titles']=[head.text]\n",
    "        sub=soup.find('div',attrs ={'class': 'flex flexResponsive flexJustifyBetween flexAlignItemsStart'})\n",
    "        story=sub.find('div',attrs={'class': 'width100 storytext'})\n",
    "        #print(story)\n",
    "        Full=\"\"\n",
    "        for row in story.findAll('p'):\n",
    "            texts=row.text\n",
    "            Full+=texts+\" \"\n",
    "        try:\n",
    "            summary=summarizer(Full)\n",
    "        except:\n",
    "            summary=[head.text]\n",
    "        #print(summary)\n",
    "        news[n]['gists']=summary\n",
    "        #news[n]['Titles']=row.text\n",
    "        date=datetime.today().strftime('%Y-%m-%d')\n",
    "        time=str(datetime.now().time())\n",
    "        \n",
    "        news[n]['Date']=date\n",
    "        news[n]['Time']=time\n",
    "\n",
    "    return news\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### India Today Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def india_today_scraper():\n",
    "    URL=\"https://www.indiatoday.in/\"\n",
    "    r=requests.get(URL)\n",
    "    #print(r)\n",
    "    soup = BeautifulSoup(r.content)  \n",
    "    #print(soup)\n",
    "    heads={}\n",
    "    count=0\n",
    "    row=soup.find('h2')\n",
    "    link=row.a['href']\n",
    "    head=row.text\n",
    "    if URL not in link:\n",
    "        link=URL+link[1:]\n",
    "    heads[head]={}\n",
    "    heads[head]['Source']='IndiaToday'\n",
    "    heads[head]['link']=link\n",
    "    #print(heads)\n",
    "    rows=soup.findAll(\"div\",attrs={'class': 'featured-post'})\n",
    "    #print(rows)\n",
    "    count=0\n",
    "    for row in rows:\n",
    "        if count==15:\n",
    "            break\n",
    "        count+=1\n",
    "        link=row.a['href']\n",
    "        head=row.text\n",
    "        if URL not in link:\n",
    "            link=URL+link[1:]\n",
    "        \n",
    "        heads[head]={}\n",
    "        heads[head]['Source']='IndiaToday'\n",
    "        heads[head]['link']=link\n",
    "        \n",
    "    sub=soup.find(\"div\",attrs={'class': 'block block-itg-widget even'})\n",
    "    rows=sub.findAll('li')\n",
    "    count=0\n",
    "    for row in rows:\n",
    "        if count==15:\n",
    "            break\n",
    "        count+=1\n",
    "        link=row.a['href']\n",
    "        head=row.text\n",
    "        if URL not in link:\n",
    "            link=URL+link[1:]\n",
    "            \n",
    "        heads[head]={}\n",
    "        heads[head]['Source']='IndiaToday'\n",
    "        heads[head]['link']=link\n",
    "        \n",
    "        \n",
    "    return heads\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_it(news):\n",
    "    for n in news.keys():\n",
    "        #print(n)\n",
    "        link=news[n]['link']\n",
    "        #print(link)\n",
    "        r=requests.get(link)\n",
    "        #print(link)\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        Briefs=[]\n",
    "        sub=soup.find(\"div\",attrs={'class': 'story-section '})\n",
    "        #print(sub)\n",
    "        flag=0\n",
    "        try:\n",
    "            h=sub.find('h1')\n",
    "            flag=1\n",
    "            news[n]['Titles']=[h.text]\n",
    "            s=[h.text]\n",
    "        except:\n",
    "            i=1\n",
    "            \n",
    "        if flag==0:\n",
    "            try:\n",
    "                flag=1\n",
    "                h=sub.find('h2')\n",
    "                news[n]['Titles']=[h.text]\n",
    "                s=[h.text]\n",
    "            except:\n",
    "                i=1\n",
    "          \n",
    "        news[n]['Titles']=[n]\n",
    "        s=[n]\n",
    "        \n",
    "        \n",
    "        sub=soup.find(\"div\",attrs={'class': 'story-left-section story-update'})\n",
    "        #print(sub)\n",
    "        Full=\"\"\n",
    "        try:\n",
    "            rows=sub.findAll('p')\n",
    "            for row in rows:\n",
    "                texts=row.text\n",
    "                #print(texts)\n",
    "                Full+=texts+\" \"\n",
    "            #print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "            summary=summarizer(Full,3)\n",
    "        except:\n",
    "            summary=s\n",
    "        #print(link)\n",
    "        #print(summary)\n",
    "        news[n]['gists']=summary\n",
    "        #news[n]['Titles']=row.text\n",
    "        date=datetime.today().strftime('%Y-%m-%d')\n",
    "        time=str(datetime.now().time())\n",
    "        \n",
    "        news[n]['Date']=date\n",
    "        news[n]['Time']=time\n",
    "        \n",
    "    return news\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News 18 scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def News_18_scraper():\n",
    "    URL=\"https://www.news18.com/\"\n",
    "    r=requests.get(URL)\n",
    "    #print(r)\n",
    "    soup = BeautifulSoup(r.content,'html5lib')  \n",
    "    #print(soup)\n",
    "    heads={}\n",
    "    sub= soup.find('div',attrs={'class': 'lead-story'})\n",
    "    #print(sub)\n",
    "    rows=sub.findAll('p')\n",
    "    #print(rows)\n",
    "    for row in rows:\n",
    "        \n",
    "        head=row.text\n",
    "        heads[head]={}\n",
    "        heads[head]['Source']='News18'\n",
    "        #print(head)\n",
    "        #print(row.a[\"href\"])\n",
    "        heads[head]['link']=row.a[\"href\"]\n",
    "        \n",
    "    sub= soup.find('ul',attrs={'class': 'lead-mstory'})\n",
    "    rows=sub.findAll('li')\n",
    "    for row in rows:\n",
    "        head=row.text\n",
    "        heads[head]={}\n",
    "        heads[head][\"Source\"]='News18'\n",
    "        heads[head][\"link\"]=row.a[\"href\"]\n",
    "    \n",
    "    return heads\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_n18(news):\n",
    "    for n in news.keys():\n",
    "        #print(n)\n",
    "        link=news[n]['link']\n",
    "        \n",
    "        r=requests.get(link)\n",
    "        #print(link)\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        Briefs=[]\n",
    "        #print(link)\n",
    "        #print(soup)\n",
    "        sub=soup.find(\"title\")\n",
    "        news[n]['Titles']=[sub.text]\n",
    "        tit=sub.text\n",
    "        flag=0\n",
    "        try:\n",
    "            flag=1\n",
    "            text=\"\"\n",
    "            sub=soup.find('div',{'class':'lbcontent paragraph'})\n",
    "            #print(sub)\n",
    "            text+=sub.text+\"\\n\"\n",
    "            sub_2=soup.find('div',{'id':'article_body'})\n",
    "            #print(sub_2.text)\n",
    "            #print(\"++++++++++++++++++++++++\")\n",
    "            text+=sub_2.text\n",
    "            summary=summarizer(text)\n",
    "           #print(summary)\n",
    "            #print(text)\n",
    "        except:\n",
    "            flag=0\n",
    "            i=1\n",
    "            \n",
    "        if flag==0:\n",
    "            text=\"\"\n",
    "            try:\n",
    "                sub=soup.find('article',{'class':'article-content-box first_big_character'})\n",
    "                rows=sub.findAll('p')\n",
    "                for row in rows:\n",
    "                    text+=row.text+\"\\n\"\n",
    "                summary=summarizer(text)\n",
    "                    \n",
    "            except:\n",
    "                summary=tit\n",
    "        #print(summary)\n",
    "        news[n]['gists']=summary\n",
    "        date=datetime.today().strftime('%Y-%m-%d')\n",
    "        time=str(datetime.now().time())\n",
    "        \n",
    "        news[n]['Date']=date\n",
    "        news[n]['Time']=time\n",
    "        \n",
    "    return news\n",
    "        \n",
    "                    \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Creater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def Merge(dict1, dict2, dict3, dict4): \n",
    "    res = {**dict1, **dict2, **dict3, **dict4} \n",
    "    return res \n",
    "\n",
    "def file_creater(date):\n",
    "    news_times=times_now_scraper()\n",
    "    times_now=extract_news_times(news_times)\n",
    "    news_rep=republic_tv_scraper()\n",
    "    republic_tv=extract_news_rep(news_rep)\n",
    "    news_it=india_today_scraper()\n",
    "    india_today=extractor_it(news_it)\n",
    "    n_18=News_18_scraper()\n",
    "    News_18=extractor_n18(n_18)\n",
    "    \n",
    "    Merged=Merge(times_now,republic_tv,india_today,News_18)\n",
    "    \n",
    "    Merged_df=pd.DataFrame(Merged)\n",
    "    Merged_df_final=Merged_df.transpose()\n",
    "    df_final=Merged_df_final.reset_index()\n",
    "    df_final_2=df_final.drop(['index'],axis=1)\n",
    "    df_final_2.to_csv('feeds/Feed_'+date+'.csv',index=False)\n",
    "    get_names('feeds/Feed_'+date+'.csv')\n",
    "    \n",
    "    return df_final_2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijit/.local/lib/python3.6/site-packages/numpy/core/_methods.py:38: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/usr/lib/python3/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /usr/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "df=file_creater('08-09-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,ast\n",
    "import pandas as pd\n",
    "def get_names(file_name):    \n",
    "    df=pd.read_csv(file_name)\n",
    "    label=[]\n",
    "    for j in range(len(df)):\n",
    "        label_temp=[]\n",
    "        try:\n",
    "            title=ast.literal_eval(df.iloc[j]['Titles'])[0]\n",
    "            doc=nlp(title)\n",
    "            \n",
    "            for X in doc.ents:\n",
    "                if X.label_ not in ('ORDINAL','TIME','DATE','MONEY','PERCENT'):\n",
    "                    label_temp.append(X.text)\n",
    "        except:\n",
    "            a=1\n",
    "        label.append(label_temp)\n",
    "    df['labels']=label\n",
    "    \n",
    "    df.to_csv(file_name,index=False)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_names('feeds/Feed_07-09-2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
